# -*- coding: utf-8 -*-
"""Credit Risk Prediction Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q9yjYfH-65boc36yVZHjSPLUr5zsfYBN

# ðŸ“¦ Step 1: Import Required Libraries
"""

# Import required libraries
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
import joblib
import shap  # For explainability

"""# ðŸ“š Step 2: Load the Dataset from Colab"""

# Upload CSV manually in Google Colab
from google.colab import files

# Upload the CSV file
uploaded = files.upload()

# Load the CSV into a DataFrame
df = pd.read_csv('nariniti_credit_score_dataset.csv')
print("âœ… Dataset Loaded Successfully!")

"""# ðŸ“Š Step 3: Data Preprocessing"""

# Define features and target variable
X = df.drop('credit_score_category', axis=1)  # Drop target column
y = df['credit_score_category']  # Target variable

# Standardize the feature values for better model performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and test sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

print("âœ… Data Preprocessing Complete!")

"""# ðŸŒ³ Step 4: Build and Train the Model"""

# Create and train the XGBoost Classifier
model = xgb.XGBClassifier(
    n_estimators=50,             # Reduce number of trees
    max_depth=4,                 # Reduce tree depth
    learning_rate=0.1,           # Slightly higher learning rate
    subsample=0.7,               # Reduce sample size for diversity
    colsample_bytree=0.7,        # Reduce feature sampling to prevent overfitting
    random_state=42
)

# Fit the model to training data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

print("âœ… XGBoost Model Training Complete!")

"""# ðŸ“ˆ Step 5: Evaluate Model Performance"""

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"ðŸŽ¯ Model Accuracy with XGBoost: {accuracy * 100:.2f}%")
print("ðŸ“Š Classification Report:\n", classification_report(y_test, y_pred))

"""# ðŸ’¾ Step 6: Save the Trained Model as a .pkl File"""

# Save the trained model as a .pkl file
model_filename = "nariniti_credit_score_model.pkl"
joblib.dump(model, model_filename)
print(f"âœ… Model saved successfully as {model_filename}")

"""Feature Importance Plot (Using SHAP with XGBoost)"""

import shap

# Explain model predictions using SHAP
explainer = shap.Explainer(model)
shap_values = explainer(X_test)

# Visualize feature importance
shap.summary_plot(shap_values, X_test, feature_names=df.drop('credit_score_category', axis=1).columns)